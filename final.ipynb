{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93b7deb4-3562-4aeb-afa4-efca05669f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#last stage - embedding matrix and transtokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4cec67e-1d35-4d61-bc27-dfa9b3e1407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your parallel corpus\n",
    "file_path = \"cleaned_parallel_corpus.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56e4b301-1b2c-4e51-b352-87908a385881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([818, 262, 3726, 1793, 2727, 262, 27636, 290, 262, 4534, 13], [3338, 266, 276, 262, 2013, 264, 413, 624, 258, 23754, 264, 1476, 366, 300, 386, 335, 266, 272, 317, 260, 292, 386, 331, 264, 297, 266, 663, 262, 268, 306]), ([464, 4534, 373, 1296, 1203, 290, 6565, 11, 290, 11854, 5017, 262, 4417, 286, 262, 2769, 13, 383, 7710, 286, 1793, 3888, 625, 262, 4417, 286, 262, 10150, 13], [284, 386, 335, 266, 272, 317, 293, 266, 268, 264, 274, 258, 278, 293, 266, 295, 317, 268, 366, 356, 384, 268, 264, 274, 389, 266, 282, 412, 350, 393, 264, 1386, 258, 260, 7509, 258, 305, 266, 260, 767, 262, 389, 266, 282, 12, 414, 2013, 264, 413, 262, 260, 783, 264, 280, 258, 1005, 262, 260, 767, 262, 518, 258, 746, 601, 266, 282, 262, 268, 306]), ([13482, 531, 11, 564, 250, 5756, 612, 307, 1657, 0, 447, 251, 290, 612, 373, 1657, 13], [420, 262, 2013, 264, 413, 817, 262, 268, 12, 458, 302, 325, 302, 270, 27, 341, 258, 276, 262, 458, 302, 488, 306])]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the English tokenizer and model\n",
    "english_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "english_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the Bengali tokenizer and model\n",
    "bengali_model_name = \"flax-community/gpt2-bengali\"\n",
    "bengali_tokenizer = AutoTokenizer.from_pretrained(bengali_model_name)\n",
    "bengali_model = AutoModelForCausalLM.from_pretrained(bengali_model_name)\n",
    "\n",
    "# Prepare lists to store tokenized sentences\n",
    "tokenized_pairs = []\n",
    "\n",
    "# Process each line in the parallel corpus\n",
    "for line in lines:\n",
    "    if \"|||\" in line:\n",
    "        english_sentence, bengali_sentence = line.split(\"|||\")\n",
    "        english_sentence = english_sentence.strip()\n",
    "        bengali_sentence = bengali_sentence.strip()\n",
    "\n",
    "        # Tokenize both sentences\n",
    "        tokenized_english = english_tokenizer.encode(english_sentence, add_special_tokens=False)\n",
    "        tokenized_bengali = bengali_tokenizer.encode(bengali_sentence, add_special_tokens=False)\n",
    "\n",
    "        # Store the tokenized pairs\n",
    "        tokenized_pairs.append((tokenized_english, tokenized_bengali))\n",
    "\n",
    "# Example output\n",
    "print(tokenized_pairs[:3])  # Show the first three tokenized pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53e9dcbb-5e2d-4ad2-a997-721fe3985baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenized pairs to a file\n",
    "with open(\"tokenized_parallel_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for english_tokens, bengali_tokens in tokenized_pairs:\n",
    "        english_tokens_str = \" \".join(map(str, english_tokens))\n",
    "        bengali_tokens_str = \" \".join(map(str, bengali_tokens))\n",
    "        f.write(f\"{english_tokens_str} ||| {bengali_tokens_str}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2d5364d-1b15-4fa0-8f82-4458c724e432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CHANGELOG.md\t\t        english_bpe.model\n",
      " CONTRIBUTING.md\t        english_bpe.vocab\n",
      " GPT_Tokenizer.ipynb\t        fast_align\n",
      " LICENSE\t\t        final.ipynb\n",
      " README.md\t\t        finetuning_llama.ipynb\n",
      " alignmenets.ipynb\t        llama3-tokenizer.js\n",
      " alignments\t\t        llama_3_1_8b_+_unsloth_2x_faster_finetuning.py\n",
      " alignments.txt\t\t        makefile\n",
      "'bangla_tokenizer(1).ipynb'     mkdocs.yml\n",
      " bangla_tokenizer.ipynb         notebooks\n",
      " bangla_tokenizer.json\t        pyproject.toml\n",
      " bidirectional.align\t        requirements.txt\n",
      " c\t\t\t        setup.cfg\n",
      " cleaned_parallel_corpus.txt    tests\n",
      " corpus.ipynb:Zone.Identifier   tokenizer_finetuning.ipynb\n",
      " daniel_smooth_mapping.ipynb    transtokenizers\n",
      " docs\t\t\t        transtokenizers.py\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4baf16c-b393-476c-999b-6db83cef35bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'fast_align'...\n",
      "remote: Enumerating objects: 213, done.\u001b[K\n",
      "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 213 (delta 2), reused 4 (delta 2), pack-reused 204\u001b[K\n",
      "Receiving objects: 100% (213/213), 70.68 KiB | 1.07 MiB/s, done.\n",
      "Resolving deltas: 100% (110/110), done.\n",
      "/workspace/transtokenizers/fast_align\n",
      "/workspace/transtokenizers/fast_align/build\n",
      "\u001b[33mCMake Warning (dev) at CMakeLists.txt:1 (project):\n",
      "  cmake_minimum_required() should be called prior to this top-level project()\n",
      "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
      "  both commands.\n",
      "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "\u001b[0m\n",
      "-- The C compiler identification is GNU 11.4.0\n",
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (cmake_minimum_required):\n",
      "  Compatibility with CMake < 3.5 will be removed from a future version of\n",
      "  CMake.\n",
      "\n",
      "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
      "  CMake that the project does not need compatibility with older versions.\n",
      "\n",
      "\u001b[0m\n",
      "-- Could NOT find SparseHash (missing: SPARSEHASH_INCLUDE_DIR) \n",
      "-- Configuring done (0.8s)\n",
      "-- Generating done (0.0s)\n",
      "-- Build files have been written to: /workspace/transtokenizers/fast_align/build\n",
      "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/fast_align.dir/src/fast_align.cc.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/fast_align.dir/src/ttables.cc.o\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX executable fast_align\u001b[0m\n",
      "[ 50%] Built target fast_align\n",
      "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/atools.dir/src/alignment_io.cc.o\u001b[0m\n",
      "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/atools.dir/src/atools.cc.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable atools\u001b[0m\n",
      "[100%] Built target atools\n",
      "/workspace/transtokenizers\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/clab/fast_align.git\n",
    "\n",
    "# Navigate to the directory and build\n",
    "%cd fast_align\n",
    "!mkdir build\n",
    "%cd build\n",
    "!cmake ..\n",
    "!make\n",
    "\n",
    "# Return to the original directory (optional)\n",
    "%cd ../..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d2639a6-39fb-4935-912e-da5057853af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARG=i\n",
      "ARG=d\n",
      "ARG=o\n",
      "ARG=v\n",
      "INITIAL PASS \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................\n",
      "expected target length = source length * 2.82151\n",
      "ITERATION 1\n",
      "...............................\n",
      "  log_e likelihood: -5.29282e+07\n",
      "  log_2 likelihood: -7.63592e+07\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.08\n",
      " posterior al-feat: -0.163033\n",
      "       size counts: 5369\n",
      "ITERATION 2\n",
      "...............................\n",
      "  log_e likelihood: -1.11038e+07\n",
      "  log_2 likelihood: -1.60194e+07\n",
      "     cross entropy: 6.27217\n",
      "        perplexity: 77.288\n",
      "      posterior p0: 0.0814341\n",
      " posterior al-feat: -0.156839\n",
      "       size counts: 5369\n",
      "  1  model al-feat: -0.054782 (tension=4)\n",
      "  2  model al-feat: -0.0655038 (tension=1.95887)\n",
      "  3  model al-feat: -0.0570081 (tension=0.132172)\n",
      "  4  model al-feat: -0.0558986 (tension=0.1)\n",
      "  5  model al-feat: -0.0558986 (tension=0.1)\n",
      "  6  model al-feat: -0.0558986 (tension=0.1)\n",
      "  7  model al-feat: -0.0558986 (tension=0.1)\n",
      "  8  model al-feat: -0.0558986 (tension=0.1)\n",
      "     final tension: 0.1\n",
      "ITERATION 3\n",
      "...............................\n",
      "  log_e likelihood: -1.06829e+07\n",
      "  log_2 likelihood: -1.54122e+07\n",
      "     cross entropy: 6.03442\n",
      "        perplexity: 65.5453\n",
      "      posterior p0: 0.0771945\n",
      " posterior al-feat: -0.280697\n",
      "       size counts: 5369\n",
      "  1  model al-feat: -0.0558986 (tension=0.1)\n",
      "  2  model al-feat: -0.0558986 (tension=0.1)\n",
      "  3  model al-feat: -0.0558986 (tension=0.1)\n",
      "  4  model al-feat: -0.0558986 (tension=0.1)\n",
      "  5  model al-feat: -0.0558986 (tension=0.1)\n",
      "  6  model al-feat: -0.0558986 (tension=0.1)\n",
      "  7  model al-feat: -0.0558986 (tension=0.1)\n",
      "  8  model al-feat: -0.0558986 (tension=0.1)\n",
      "     final tension: 0.1\n",
      "ITERATION 4\n",
      "...............................\n",
      "  log_e likelihood: -1.04966e+07\n",
      "  log_2 likelihood: -1.51434e+07\n",
      "     cross entropy: 5.92916\n",
      "        perplexity: 60.9335\n",
      "      posterior p0: 0.0766159\n",
      " posterior al-feat: -0.277567\n",
      "       size counts: 5369\n",
      "  1  model al-feat: -0.0558986 (tension=0.1)\n",
      "  2  model al-feat: -0.0558986 (tension=0.1)\n",
      "  3  model al-feat: -0.0558986 (tension=0.1)\n",
      "  4  model al-feat: -0.0558986 (tension=0.1)\n",
      "  5  model al-feat: -0.0558986 (tension=0.1)\n",
      "  6  model al-feat: -0.0558986 (tension=0.1)\n",
      "  7  model al-feat: -0.0558986 (tension=0.1)\n",
      "  8  model al-feat: -0.0558986 (tension=0.1)\n",
      "     final tension: 0.1\n",
      "ITERATION 5 (FINAL)\n",
      "...............................\n",
      "  log_e likelihood: -1.04108e+07\n",
      "  log_2 likelihood: -1.50196e+07\n",
      "     cross entropy: 5.8807\n",
      "        perplexity: 58.9206\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 5369\n"
     ]
    }
   ],
   "source": [
    "!fast_align/build/fast_align -i tokenized_parallel_corpus.txt -d -o -v > aligned_tokens.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9afb095-e54e-4e2e-abf7-b85b3205b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def parse_alignment_file(filepath):\n",
    "    token_map = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            pairs = line.strip().split()\n",
    "            for pair in pairs:\n",
    "                english_token_index, bengali_token_index = map(int, pair.split('-'))\n",
    "                token_map[bengali_token_index][english_token_index] += 1\n",
    "    \n",
    "    return token_map\n",
    "\n",
    "# Path to the aligned tokens file\n",
    "aligned_tokens_path = \"aligned_tokens.txt\"\n",
    "\n",
    "# Parse the file to get the token mapping\n",
    "token_mapping = parse_alignment_file(aligned_tokens_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a921611-7e8d-4884-b03c-3764352397ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def remap_model_embeddings(english_model, bengali_model, token_mapping):\n",
    "    with torch.no_grad():\n",
    "        english_embeddings = english_model.get_input_embeddings().weight.data\n",
    "        bengali_embeddings = bengali_model.get_input_embeddings().weight.data\n",
    "        \n",
    "        for b_token_index, e_token_indices in token_mapping.items():\n",
    "            total_weight = sum(e_token_indices.values())\n",
    "            weighted_embedding = sum(english_embeddings[e_token_index] * (weight / total_weight) for e_token_index, weight in e_token_indices.items())\n",
    "            bengali_embeddings[b_token_index] = weighted_embedding\n",
    "\n",
    "# Remap the embeddings of the Bengali model\n",
    "remap_model_embeddings(english_model, bengali_model, token_mapping)\n",
    "\n",
    "# Save the remapped model\n",
    "output_path = \"remapped_bengali_model\"\n",
    "bengali_model.save_pretrained(output_path)\n",
    "bengali_tokenizer.save_pretrained(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31510582-59ff-45d3-9570-545b81739738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-09 12:32:54,786] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('remapped_bengali_model/tokenizer_config.json',\n",
       " 'remapped_bengali_model/special_tokens_map.json',\n",
       " 'remapped_bengali_model/vocab.json',\n",
       " 'remapped_bengali_model/merges.txt',\n",
       " 'remapped_bengali_model/added_tokens.json',\n",
       " 'remapped_bengali_model/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def remap_model_embeddings(english_model, bengali_model, token_mapping):\n",
    "    with torch.no_grad():\n",
    "        english_embeddings = english_model.get_input_embeddings().weight.data\n",
    "        bengali_embeddings = bengali_model.get_input_embeddings().weight.data\n",
    "        \n",
    "        for b_token_index, e_token_indices in token_mapping.items():\n",
    "            total_weight = sum(e_token_indices.values())\n",
    "            weighted_embedding = sum(english_embeddings[e_token_index] * (weight / total_weight) for e_token_index, weight in e_token_indices.items())\n",
    "            bengali_embeddings[b_token_index] = weighted_embedding\n",
    "\n",
    "# Remap the embeddings of the Bengali model\n",
    "remap_model_embeddings(english_model, bengali_model, token_mapping)\n",
    "\n",
    "# Save the remapped model\n",
    "output_path = \"remapped_bengali_model\"\n",
    "bengali_model.save_pretrained(output_path)\n",
    "bengali_tokenizer.save_pretrained(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1631feaf-2668-460b-8c07-9de0e20efdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "আদিতে ঈশ্বর আকাশমণ্ডল ও পৃথিবীর সৃষ্টি করলেন। হযরত মূসা, ঈসা, আঃ ও মৃত ফৌজ তাঁহ ন\n"
     ]
    }
   ],
   "source": [
    "input_text = \"আদিতে ঈশ্বর আকাশমণ্ডল ও পৃথিবীর সৃষ্টি করলেন।\"\n",
    "inputs = bengali_tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Set the number of new tokens you want the model to generate\n",
    "max_new_tokens = 20\n",
    "\n",
    "outputs = bengali_model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=20,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.2  # Apply a repetition penalty\n",
    ")\n",
    "\n",
    "print(bengali_tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1bd5e90-801b-4a5c-871b-43d282e6921e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31082it [00:11, 2622.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned corpus created at: transtokenizers/aligned_corpus.moses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "from pathlib import Path\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "ALIGNMENT_UNIT = \"WORDS\"  # \"WORDS\" to ensure proper alignment for Bengali, which is non-ideographic\n",
    "home_path = Path(\"transtokenizers\")  # Update this path as needed for your environment\n",
    "\n",
    "def create_aligned_corpus(\n",
    "    source_language: str,\n",
    "    target_language: str,\n",
    "    source_tokenizer: str,\n",
    "    target_tokenizer: str,\n",
    "    cleaned_parallel_corpus: str\n",
    "):\n",
    "    OLD_TOKENIZER_FRIENDLY_NAME = source_tokenizer.replace('/', '--')\n",
    "    NEW_TOKENIZER_FRIENDLY_NAME = target_tokenizer.replace('/', '--')\n",
    "\n",
    "    # Load tokenizers for the two models\n",
    "    old_tokenizer = AutoTokenizer.from_pretrained(source_tokenizer)\n",
    "    new_tokenizer = AutoTokenizer.from_pretrained(target_tokenizer)\n",
    "\n",
    "    # Set prefixes for tokenizers\n",
    "    OLD_TOKENIZER_1ST_PREFIX = old_tokenizer.convert_ids_to_tokens(\n",
    "        old_tokenizer.encode(\" a\", add_special_tokens=False)[0]\n",
    "    ).rstrip(\"a\")\n",
    "    NEW_TOKENIZER_1ST_PREFIX = new_tokenizer.convert_ids_to_tokens(\n",
    "        new_tokenizer.encode(\" a\", add_special_tokens=False)[0]\n",
    "    ).rstrip(\"a\")\n",
    "    OLD_TOKENIZER_2ND_PREFIX = old_tokenizer.convert_ids_to_tokens(\n",
    "        old_tokenizer.encode(\"aaaaaaaaaaaaaaaaaaaaaa\", add_special_tokens=False)[1]\n",
    "    ).rstrip('a')\n",
    "    NEW_TOKENIZER_2ND_PREFIX = new_tokenizer.convert_ids_to_tokens(\n",
    "        new_tokenizer.encode(\"aaaaaaaaaaaaaaaaaaaaaa\", add_special_tokens=False)[1]\n",
    "    ).rstrip('a')\n",
    "\n",
    "    out_path = f'{home_path}/aligned_corpus.moses'\n",
    "\n",
    "    if os.path.exists(out_path):\n",
    "        print(f'Data already preprocessed for fast_align')\n",
    "    else:\n",
    "        os.makedirs(f'{home_path}/alignments', exist_ok=True)\n",
    "        with open(cleaned_parallel_corpus, 'r') as infile, open(out_path, 'w') as outfile:\n",
    "            for line in tqdm(infile):\n",
    "                if '|||' in line:\n",
    "                    line_source, line_target = line.strip().split(' ||| ')\n",
    "                    \n",
    "                    # Tokenize and merge tokens\n",
    "                    if ALIGNMENT_UNIT == 'WORDS':\n",
    "                        line1 = re.sub(\n",
    "                            r'(?!'\n",
    "                            + OLD_TOKENIZER_1ST_PREFIX\n",
    "                            + r')(\\p{L})[ ](?!'\n",
    "                            + OLD_TOKENIZER_1ST_PREFIX\n",
    "                            + r')(?='\n",
    "                            + OLD_TOKENIZER_2ND_PREFIX\n",
    "                            + r'\\p{L})',\n",
    "                            r'\\1—',\n",
    "                            ' '.join(old_tokenizer.tokenize(line_source.strip())),\n",
    "                        )\n",
    "                        line2 = re.sub(\n",
    "                            r'(?!'\n",
    "                            + NEW_TOKENIZER_1ST_PREFIX\n",
    "                            + r')(\\p{L})[ ](?!'\n",
    "                            + NEW_TOKENIZER_1ST_PREFIX\n",
    "                            + r')(?='\n",
    "                            + NEW_TOKENIZER_2ND_PREFIX\n",
    "                            + r'\\p{L})',\n",
    "                            r'\\1—',\n",
    "                            ' '.join(new_tokenizer.tokenize(line_target.strip())),\n",
    "                        )\n",
    "\n",
    "                    # Write the tokenized and aligned lines to the output file\n",
    "                    outfile.write(line1.strip() + ' ||| ' + line2.strip() + '\\n')\n",
    "\n",
    "    return out_path\n",
    "\n",
    "# Example usage\n",
    "source_language = \"en\"\n",
    "target_language = \"bn\"\n",
    "source_tokenizer = \"gpt2\"  # English tokenizer\n",
    "target_tokenizer = \"flax-community/gpt2-bengali\"  # Bengali tokenizer\n",
    "cleaned_parallel_corpus = \"cleaned_parallel_corpus.txt\"  # Path to your cleaned corpus\n",
    "\n",
    "aligned_corpus_path = create_aligned_corpus(\n",
    "    source_language=source_language,\n",
    "    target_language=target_language,\n",
    "    source_tokenizer=source_tokenizer,\n",
    "    target_tokenizer=target_tokenizer,\n",
    "    cleaned_parallel_corpus=cleaned_parallel_corpus\n",
    ")\n",
    "\n",
    "print(f\"Aligned corpus created at: {aligned_corpus_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13f67d82-ae39-457b-9ad9-176abc27a6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. Output saved to utokenized_corpus.txt\n"
     ]
    }
   ],
   "source": [
    "#working with utoken\n",
    "from utoken import utokenize\n",
    "\n",
    "# Initialize Utoken for both English and Bengali\n",
    "english_tokenizer = utokenize.Tokenizer(lang_code='eng')\n",
    "bengali_tokenizer = utokenize.Tokenizer(lang_code='ben')\n",
    "\n",
    "# Function to process the corpus file\n",
    "def tokenize_corpus(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            if '|||' in line:\n",
    "                english_part, bengali_part = line.split(' ||| ')\n",
    "                english_tokens = english_tokenizer.utokenize_string(english_part.strip())\n",
    "                bengali_tokens = bengali_tokenizer.utokenize_string(bengali_part.strip())\n",
    "                outfile.write(f\"{english_tokens} ||| {bengali_tokens}\\n\")\n",
    "\n",
    "# Path to your cleaned parallel corpus\n",
    "input_file = 'cleaned_parallel_corpus.txt'\n",
    "output_file = 'utokenized_corpus.txt'\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenize_corpus(input_file, output_file)\n",
    "\n",
    "print(\"Tokenization complete. Output saved to utokenized_corpus.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbe53b9a-97a1-403b-b62c-de080b7c9510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying with my own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c99bbd4-4aac-41dc-98b6-d476e388cb1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PreTrainedTokenizerFast' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the Bengali LLaMA 3.1 8B Tokenizer\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mPreTrainedTokenizerFast\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubhrokomol/Bengali-Llama-3.1-8B-Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Example texts\u001b[39;00m\n\u001b[1;32m      7\u001b[0m english_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the beginning God created the heavens and the earth.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PreTrainedTokenizerFast' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "\n",
    "# Load the Bengali LLaMA 3.1 8B Tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"subhrokomol/Bengali-Llama-3.1-8B-Tokenizer\")\n",
    "\n",
    "# Example texts\n",
    "english_text = \"In the beginning God created the heavens and the earth.\"\n",
    "bengali_text = \"আদিতে ঈশ্বর আকাশমণ্ডল ও পৃথিবীর সৃষ্টি করলেন ।\"\n",
    "\n",
    "# Tokenize the texts\n",
    "english_tokens = tokenizer.tokenize(english_text)\n",
    "bengali_tokens = tokenizer.tokenize(bengali_text)\n",
    "\n",
    "print(\"English Tokens:\", english_tokens)\n",
    "print(\"Bengali Tokens:\", bengali_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26ba34a2-a40d-4b79-9c87-dcce021873ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the dutch tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7163ee2-738f-4bcb-a6c5-e60dff64a3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299aabc8531d4353b35603adfaf39220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/207 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154ff02fad52495a96e99ca294119fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a062365d2c9b468f9dd61cf6abaa4b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/499k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ba7043be9840cd9817426e2feec7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.89M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc7c64f73e44a22847b9ab351cb6cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b849cf181b2046d18bba5ed1b98edd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Tokens: ['In', 'Ġthe', 'Ġbeg', 'inning', 'ĠGod', 'Ġcre', 'ated', 'Ġthe', 'Ġhe', 'aven', 's', 'Ġand', 'Ġthe', 'Ġe', 'arth', '.']\n",
      "Dutch Tokens: ['In', 'Ġhet', 'Ġbegin', 'Ġschiep', 'ĠGod', 'Ġde', 'Ġhemel', 'Ġen', 'Ġde', 'Ġaarde', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the Dutch GPT-Neo tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yhavinga/gpt-neo-1.3B-dutch\")\n",
    "\n",
    "# Example sentences\n",
    "english_text = \"In the beginning God created the heavens and the earth.\"\n",
    "dutch_text = \"In het begin schiep God de hemel en de aarde.\"\n",
    "\n",
    "# Tokenize the texts\n",
    "english_tokens = tokenizer.tokenize(english_text)\n",
    "dutch_tokens = tokenizer.tokenize(dutch_text)\n",
    "\n",
    "print(\"English Tokens:\", english_tokens)\n",
    "print(\"Dutch Tokens:\", dutch_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08d96dd9-8c6c-459e-815c-2590b12d71f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁আদি', 'তে', '▁ঈশ্বর', '▁আকাশমণ্ডল', '▁ও', '▁পৃথিবীর', '▁সৃষ্টি', '▁করলেন', '▁', '।']\n",
      "আদিতে ঈশ্বর আকাশমণ্ডল ও পৃথিবীর সৃষ্টি করলেন ।\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ben-ben2017.txt\n",
      "  input_format: \n",
      "  model_prefix: bengali_sp\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16158\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ben-ben2017.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 31099 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=3752317\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9641% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=68\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999641\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 31099 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=1998148\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 52828 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 31099\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 38765\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 38765 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19508 obj=10.8319 num_tokens=73973 num_tokens/piece=3.79193\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=16153 obj=8.71563 num_tokens=74002 num_tokens/piece=4.58132\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: bengali_sp.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: bengali_sp.vocab\n"
     ]
    }
   ],
   "source": [
    "#trying sentencepiece \n",
    "import sentencepiece as spm\n",
    "\n",
    "# Train a SentencePiece model\n",
    "spm.SentencePieceTrainer.train(input='ben-ben2017.txt', model_prefix='bengali_sp', vocab_size=16158, model_type='unigram', character_coverage=0.9995)\n",
    "\n",
    "# Load the model\n",
    "sp = spm.SentencePieceProcessor(model_file='bengali_sp.model')\n",
    "\n",
    "# Encode and decode\n",
    "tokens = sp.encode('আদিতে ঈশ্বর আকাশমণ্ডল ও পৃথিবীর সৃষ্টি করলেন ।', out_type=str)\n",
    "print(tokens)\n",
    "\n",
    "# Decode\n",
    "decoded_text = sp.decode(tokens)\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bd63205-9681-4532-81df-2a2100e75ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3cc0d3f-69db-4817-b7d5-179372b7884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁অনুযায়ী', '▁আমার', '▁খাবে']\n",
      "Decoded Text: অনুযায়ী আমার খাবে\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Load the SentencePiece model\n",
    "sp = spm.SentencePieceProcessor(model_file='bengali_sp.model')\n",
    "\n",
    "# Tokenize text\n",
    "bengali_text = \"অনুযায়ী আমার খাবে\"\n",
    "tokens = sp.encode_as_pieces(bengali_text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Decode tokens back to text\n",
    "decoded_text = sp.decode_pieces(tokens)\n",
    "print(\"Decoded Text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9c0bc39-df56-4830-8e74-f279fe005c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the sentencepiece model for creating the moses file to be used for fast align\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4371d582-8ab0-4c8e-b8bf-11c71d44a1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31082it [00:04, 6452.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned corpus created at: sentencepiece/aligned_corpus.moses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "from pathlib import Path\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "ALIGNMENT_UNIT = \"WORDS\"\n",
    "home_path = Path(\"sentencepiece\")\n",
    "\n",
    "def create_aligned_corpus(\n",
    "    source_language: str,\n",
    "    target_language: str,\n",
    "    source_tokenizer: str,\n",
    "    sentencepiece_model: str,  # Path to SentencePiece model for Bengali\n",
    "    cleaned_parallel_corpus: str\n",
    "):\n",
    "    OLD_TOKENIZER_FRIENDLY_NAME = source_tokenizer.replace('/', '--')\n",
    "    NEW_TOKENIZER_FRIENDLY_NAME = \"SentencePiece-Bengali\"\n",
    "\n",
    "    # Load the English tokenizer\n",
    "    old_tokenizer = AutoTokenizer.from_pretrained(source_tokenizer)\n",
    "    \n",
    "    # Load the SentencePiece model for Bengali\n",
    "    sp = spm.SentencePieceProcessor(model_file=sentencepiece_model)\n",
    "\n",
    "    # Set prefixes for tokenizers (these may be ignored in the SentencePiece processing)\n",
    "    OLD_TOKENIZER_1ST_PREFIX = old_tokenizer.convert_ids_to_tokens(\n",
    "        old_tokenizer.encode(\" a\", add_special_tokens=False)[0]\n",
    "    ).rstrip(\"a\")\n",
    "    \n",
    "    OLD_TOKENIZER_2ND_PREFIX = old_tokenizer.convert_ids_to_tokens(\n",
    "        old_tokenizer.encode(\"aaaaaaaaaaaaaaaaaaaaaa\", add_special_tokens=False)[1]\n",
    "    ).rstrip('a')\n",
    "\n",
    "    out_path = f'{home_path}/aligned_corpus.moses'\n",
    "\n",
    "    if os.path.exists(out_path):\n",
    "        print(f'Data already preprocessed for fast_align')\n",
    "    else:\n",
    "        os.makedirs(f'{home_path}/alignments', exist_ok=True)\n",
    "        with open(cleaned_parallel_corpus, 'r', encoding='utf-8') as infile, open(out_path, 'w', encoding='utf-8') as outfile:\n",
    "            for line in tqdm(infile):\n",
    "                if '|||' in line:\n",
    "                    line_source, line_target = line.strip().split(' ||| ')\n",
    "                    \n",
    "                    # Tokenize and merge tokens\n",
    "                    if ALIGNMENT_UNIT == 'WORDS':\n",
    "                        # Tokenize using the English GPT-2 tokenizer\n",
    "                        line1 = re.sub(\n",
    "                            r'(?!'\n",
    "                            + OLD_TOKENIZER_1ST_PREFIX\n",
    "                            + r')(\\p{L})[ ](?!'\n",
    "                            + OLD_TOKENIZER_1ST_PREFIX\n",
    "                            + r')(?='\n",
    "                            + OLD_TOKENIZER_2ND_PREFIX\n",
    "                            + r'\\p{L})',\n",
    "                            r'\\1—',\n",
    "                            ' '.join(old_tokenizer.tokenize(line_source.strip())),\n",
    "                        )\n",
    "                        # Tokenize using the SentencePiece tokenizer for Bengali\n",
    "                        line2 = ' '.join(sp.encode_as_pieces(line_target.strip()))\n",
    "\n",
    "                    # Write the tokenized and aligned lines to the output file\n",
    "                    outfile.write(line1.strip() + ' ||| ' + line2.strip() + '\\n')\n",
    "\n",
    "    return out_path\n",
    "\n",
    "# Example usage\n",
    "source_language = \"en\"\n",
    "target_language = \"bn\"\n",
    "source_tokenizer = \"gpt2\"  # English tokenizer\n",
    "sentencepiece_model = \"bengali_sp.model\"  # Path to your SentencePiece model\n",
    "cleaned_parallel_corpus = \"cleaned_parallel_corpus.txt\"  # Path to your cleaned corpus\n",
    "\n",
    "aligned_corpus_path = create_aligned_corpus(\n",
    "    source_language=source_language,\n",
    "    target_language=target_language,\n",
    "    source_tokenizer=source_tokenizer,\n",
    "    sentencepiece_model=sentencepiece_model,\n",
    "    cleaned_parallel_corpus=cleaned_parallel_corpus\n",
    ")\n",
    "\n",
    "print(f\"Aligned corpus created at: {aligned_corpus_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bf492392-c978-4f4d-95cb-684463e4b4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARG=i\n",
      "ARG=d\n",
      "ARG=o\n",
      "ARG=v\n",
      "INITIAL PASS \n",
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "expected target length = source length * 0.960772\n",
      "ITERATION 1\n",
      "...............................\n",
      "  log_e likelihood: -1.65582e+07\n",
      "  log_2 likelihood: -2.38884e+07\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.08\n",
      " posterior al-feat: -0.166082\n",
      "       size counts: 1992\n",
      "ITERATION 2\n",
      "...............................\n",
      "  log_e likelihood: -5.28625e+06\n",
      "  log_2 likelihood: -7.62644e+06\n",
      "     cross entropy: 9.54481\n",
      "        perplexity: 746.917\n",
      "      posterior p0: 0.0805165\n",
      " posterior al-feat: -0.145953\n",
      "       size counts: 1992\n",
      "  1  model al-feat: -0.184488 (tension=4)\n",
      "  2  model al-feat: -0.165357 (tension=4.77071)\n",
      "  3  model al-feat: -0.156903 (tension=5.1588)\n",
      "  4  model al-feat: -0.152446 (tension=5.37782)\n",
      "  5  model al-feat: -0.149904 (tension=5.50769)\n",
      "  6  model al-feat: -0.148392 (tension=5.58671)\n",
      "  7  model al-feat: -0.147472 (tension=5.6355)\n",
      "  8  model al-feat: -0.146904 (tension=5.66588)\n",
      "     final tension: 5.68491\n",
      "ITERATION 3\n",
      "...............................\n",
      "  log_e likelihood: -4.3303e+06\n",
      "  log_2 likelihood: -6.24731e+06\n",
      "     cross entropy: 7.81876\n",
      "        perplexity: 225.778\n",
      "      posterior p0: 0.0578756\n",
      " posterior al-feat: -0.126848\n",
      "       size counts: 1992\n",
      "  1  model al-feat: -0.14655 (tension=5.68491)\n",
      "  2  model al-feat: -0.139544 (tension=6.07894)\n",
      "  3  model al-feat: -0.135337 (tension=6.33285)\n",
      "  4  model al-feat: -0.132649 (tension=6.50262)\n",
      "  5  model al-feat: -0.130867 (tension=6.61862)\n",
      "  6  model al-feat: -0.129657 (tension=6.69898)\n",
      "  7  model al-feat: -0.128824 (tension=6.75516)\n",
      "  8  model al-feat: -0.128244 (tension=6.79468)\n",
      "     final tension: 6.82258\n",
      "ITERATION 4\n",
      "...............................\n",
      "  log_e likelihood: -4.12808e+06\n",
      "  log_2 likelihood: -5.95557e+06\n",
      "     cross entropy: 7.45364\n",
      "        perplexity: 175.294\n",
      "      posterior p0: 0.0571801\n",
      " posterior al-feat: -0.118666\n",
      "       size counts: 1992\n",
      "  1  model al-feat: -0.127837 (tension=6.82258)\n",
      "  2  model al-feat: -0.12522 (tension=7.00601)\n",
      "  3  model al-feat: -0.12341 (tension=7.1371)\n",
      "  4  model al-feat: -0.122129 (tension=7.23198)\n",
      "  5  model al-feat: -0.12121 (tension=7.30125)\n",
      "  6  model al-feat: -0.120543 (tension=7.35213)\n",
      "  7  model al-feat: -0.120055 (tension=7.38968)\n",
      "  8  model al-feat: -0.119696 (tension=7.41746)\n",
      "     final tension: 7.43807\n",
      "ITERATION 5 (FINAL)\n",
      "...............................\n",
      "  log_e likelihood: -4.06789e+06\n",
      "  log_2 likelihood: -5.86873e+06\n",
      "     cross entropy: 7.34495\n",
      "        perplexity: 162.574\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 1992\n"
     ]
    }
   ],
   "source": [
    "!./fast_align/build/fast_align -i fast_align/aligned_corpus.moses -d -o -v > fast_align/aligned_output.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d063ce-b808-4c28-8ba1-88dd6ced10d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#token re-encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e5edb79-1571-42c4-a61e-e6c5d146e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def map_tokens(mapped_tokens_file: str, source_tokenizer: str, sentencepiece_model: str):\n",
    "    print(\"Mapping tokens\")\n",
    "\n",
    "    # English tokenizer\n",
    "    old_tokenizer = AutoTokenizer.from_pretrained(source_tokenizer)\n",
    "\n",
    "    # SentencePiece model\n",
    "    sp = spm.SentencePieceProcessor(model_file=sentencepiece_model)\n",
    "\n",
    "    # Save the vocabularies in a set for improved performance\n",
    "    old_tokenizer_vocab = set(old_tokenizer.vocab.keys())\n",
    "    new_tokenizer_vocab = set(sp.id_to_piece(id) for id in range(sp.get_piece_size()))\n",
    "\n",
    "    tokenized_possible_translations = defaultdict(lambda: defaultdict(int))\n",
    "    untokenized_possible_translations = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    def add_token_pair(count, new_token, old_token):\n",
    "        tokenized_possible_translations[new_token][old_token] += count\n",
    "\n",
    "    def add_word_pair(count, new_word, old_word, all_to_all_mapping=False):\n",
    "        old_word_tokenized = old_word.split('—')\n",
    "        new_word_tokenized = new_word.split('—')\n",
    "\n",
    "        if all_to_all_mapping:\n",
    "            count_dilution = len(old_word_tokenized)\n",
    "            old_word_tokenized = np.tile(old_word_tokenized, len(new_word_tokenized))\n",
    "            new_word_tokenized = np.repeat(new_word_tokenized, count_dilution)\n",
    "        elif len(old_word_tokenized) != len(new_word_tokenized):\n",
    "            gcd = np.gcd(len(old_word_tokenized), len(new_word_tokenized))\n",
    "            count_dilution = len(old_word_tokenized) // gcd\n",
    "            old_word_tokenized = np.repeat(old_word_tokenized, len(new_word_tokenized) // gcd)\n",
    "            new_word_tokenized = np.repeat(new_word_tokenized, count_dilution)\n",
    "\n",
    "        for token_old, token_new in zip(old_word_tokenized, new_word_tokenized):\n",
    "            tokenized_possible_translations[token_new][token_old] += max(1, count // count_dilution)\n",
    "\n",
    "    total_alignments = 0\n",
    "    with open(mapped_tokens_file) as f:\n",
    "        for line in f:\n",
    "            total_alignments += 1\n",
    "\n",
    "    with open(mapped_tokens_file) as f:\n",
    "        for line in tqdm(f, total=total_alignments):\n",
    "            line = line.rstrip('\\n')\n",
    "            if line == '':\n",
    "                continue\n",
    "\n",
    "            old_word, new_word, log_prob, count = line.split('\\t')\n",
    "            count = int(float(count))\n",
    "\n",
    "            if count < 10:\n",
    "                continue\n",
    "\n",
    "            if old_word == '<eps>' or new_word == '<eps>':\n",
    "                continue\n",
    "\n",
    "            if (ALIGNMENT_UNIT != 'WORDS') or ((new_word in new_tokenizer_vocab) and (old_word in old_tokenizer_vocab)):\n",
    "                add_token_pair(count, new_word, old_word)\n",
    "            else:\n",
    "                half_count = max(1, count // 2)\n",
    "                add_word_pair(half_count, new_word, old_word, all_to_all_mapping=True)\n",
    "                add_word_pair(half_count, new_word, old_word, all_to_all_mapping=False)\n",
    "\n",
    "            untokenized_possible_translations[new_word][old_word] += count\n",
    "\n",
    "    print(f'Number of tokens with a translation: {len(tokenized_possible_translations)}')\n",
    "    print(f'Number of new tokens: {len(new_tokenizer_vocab)}')\n",
    "    print(f'Percentage of tokens with a translation: {int(len(tokenized_possible_translations) / len(new_tokenizer_vocab) * 1000)/10}%')\n",
    "\n",
    "    return tokenized_possible_translations, untokenized_possible_translations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "74010194-9ac3-4916-84af-86b25c2bb557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e771a2b8-be97-4dd4-945a-904e864d8ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31082 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m source_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m sentencepiece_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbengali_sp.model\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Path to your SentencePiece model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m tokenized_possible_translations, untokenized_possible_translations \u001b[38;5;241m=\u001b[39m \u001b[43mmap_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapped_tokens_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentencepiece_model\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Print a sample of the token mappings\u001b[39;00m\n\u001b[1;32m     11\u001b[0m sample_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "Cell \u001b[0;32mIn[58], line 53\u001b[0m, in \u001b[0;36mmap_tokens\u001b[0;34m(mapped_tokens_file, source_tokenizer, sentencepiece_model)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m line \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m old_word, new_word, log_prob, count \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(count))\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "# Run the map_tokens function\n",
    "mapped_tokens_file = \"fast_align/aligned_output.txt\"  # Replace with your actual file path\n",
    "source_tokenizer = \"gpt2\"\n",
    "sentencepiece_model = \"bengali_sp.model\"  # Path to your SentencePiece model\n",
    "\n",
    "tokenized_possible_translations, untokenized_possible_translations = map_tokens(\n",
    "    mapped_tokens_file, source_tokenizer, sentencepiece_model\n",
    ")\n",
    "\n",
    "# Print a sample of the token mappings\n",
    "sample_size = 10\n",
    "print(\"Sample of Token Mappings:\")\n",
    "for i, (new_token, mappings) in enumerate(tokenized_possible_translations.items()):\n",
    "    if i >= sample_size:\n",
    "        break\n",
    "    print(f\"{new_token} -> {dict(mappings)}\")\n",
    "\n",
    "# Verify mappings by comparing with known or expected pairs\n",
    "# Example: Check if the Bengali token for \"light\" is mapped correctly to the English token\n",
    "expected_mapping = \"আলো\"  # Replace with the Bengali token for \"light\"\n",
    "if expected_mapping in tokenized_possible_translations:\n",
    "    print(f\"Mapping for '{expected_mapping}': {tokenized_possible_translations[expected_mapping]}\")\n",
    "else:\n",
    "    print(f\"No mapping found for '{expected_mapping}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "15f4f6ce-896c-4457-9484-ead12875e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tokens(mapped_tokens_file: str, source_tokenizer: str, sentencepiece_model: str):\n",
    "    print(\"Mapping tokens\")\n",
    "\n",
    "    # Load the English tokenizer\n",
    "    old_tokenizer = AutoTokenizer.from_pretrained(source_tokenizer)\n",
    "\n",
    "    # Load the Bengali SentencePiece model\n",
    "    sp = spm.SentencePieceProcessor(model_file=sentencepiece_model)\n",
    "\n",
    "    tokenized_possible_translations = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    with open(mapped_tokens_file) as f:\n",
    "        for line in tqdm(f):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Split the line by spaces to get the alignment pairs\n",
    "            alignments = line.split()\n",
    "\n",
    "            # For each alignment pair, get the corresponding tokens\n",
    "            for alignment in alignments:\n",
    "                try:\n",
    "                    source_idx, target_idx = map(int, alignment.split('-'))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "                # Get the tokens from the respective tokenizers\n",
    "                old_token = old_tokenizer.decode([source_idx]).strip()\n",
    "                new_token = sp.id_to_piece(target_idx)\n",
    "\n",
    "                # Increment the mapping count\n",
    "                tokenized_possible_translations[new_token][old_token] += 1\n",
    "\n",
    "    print(f'Number of tokens with a translation: {len(tokenized_possible_translations)}')\n",
    "    print(f'Percentage of tokens with a translation: {int(len(tokenized_possible_translations) / sp.get_piece_size() * 1000)/10}%')\n",
    "\n",
    "    return tokenized_possible_translations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ea023d01-03fa-4d99-aafd-269ba0fa9c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31082it [00:06, 4981.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens with a translation: 89\n",
      "Percentage of tokens with a translation: 0.5%\n",
      "<unk> -> {'#': 2103, '\"': 4448, '!': 17663, \"'\": 644, '*': 266, '%': 1113, '1': 57, '$': 1586, ',': 177, '&': 709, '(': 454, '+': 219, ')': 312, '.': 101, '3': 43, '6': 29, '/': 72, '<': 7, '-': 123, '4': 31, '8': 20, '5': 28, '0': 66, '=': 5, '7': 18, '2': 44, ':': 15, ';': 7, '9': 15, '>': 4, 'F': 3, 'B': 4, '@': 2, 'E': 2, 'O': 3, 'G': 2, 'I': 2, 'D': 2, 'C': 1, '?': 3, 'A': 1}\n",
      "<s> -> {'!': 6219, '$': 3253, '\"': 6992, '.': 190, '(': 921, \"'\": 1172, '%': 2346, '#': 4715, '&': 1716, '*': 540, '+': 378, '0': 122, ')': 683, '7': 21, '5': 44, '/': 146, ':': 15, '2': 90, ',': 295, '3': 67, '-': 228, '1': 96, '4': 55, '<': 9, '9': 15, ';': 15, 'H': 2, '6': 33, 'A': 5, '8': 18, '=': 5, 'B': 4, '>': 11, '?': 7, 'D': 1, 'J': 2, 'E': 3, 'C': 1, 'F': 3, 'Z': 1, 'S': 1, '@': 2, 'I': 2, 'G': 2, 'P': 1, 'N': 1, 'O': 1}\n",
      "</s> -> {'$': 4905, '%': 3632, '\"': 4258, '!': 2363, '&': 2618, \"'\": 1828, '-': 298, '(': 1327, '#': 5360, '/': 180, '0': 163, '+': 552, '2': 84, ')': 918, '*': 671, ',': 403, '.': 224, ';': 11, '1': 119, '6': 41, '>': 10, '8': 31, '3': 66, '4': 58, '<': 18, '@': 8, '9': 17, '5': 41, '7': 30, '=': 9, '?': 10, 'A': 5, 'B': 7, ':': 14, 'G': 1, 'Q': 1, 'V': 1, 'H': 1, 'D': 1, 'E': 1, 'O': 1, 'R': 1}\n",
      ", -> {\"'\": 2419, '#': 4163, '\"': 2570, '$': 4985, '%': 4508, ')': 1256, '+': 652, '1': 135, '*': 868, '&': 3397, '(': 1690, '.': 298, '?': 12, '!': 1509, '9': 24, ',': 487, '3': 76, '5': 62, '0': 155, '-': 337, '7': 21, '2': 94, '/': 182, '<': 18, '4': 77, '8': 25, ';': 10, 'A': 4, '6': 31, '=': 11, 'B': 2, ':': 16, 'E': 2, 'P': 3, '@': 7, 'I': 2, 'M': 2, 'O': 2, 'D': 4, 'U': 1, 'Q': 1, 'H': 1, 'F': 2, '>': 3, 'W': 1, 'Y': 1, 'R': 1, 'J': 1, 'C': 2}\n",
      "। -> {'(': 2341, '%': 4472, \"'\": 3284, '\"': 1763, '$': 3882, ')': 1613, '+': 855, '&': 4012, ',': 655, '0': 207, '#': 2904, '*': 1188, '!': 1078, '/': 261, '.': 356, 'G': 3, '5': 49, '2': 121, '-': 424, '9': 24, '3': 98, '4': 67, ';': 13, ':': 22, '6': 48, '1': 161, '8': 33, '?': 13, '7': 31, 'H': 1, '<': 10, '=': 9, '>': 9, 'E': 4, 'A': 3, 'L': 1, '@': 9, 'F': 3, 'I': 1, 'Q': 1, 'R': 1, 'D': 1, 'J': 1, 'C': 1}\n",
      "ের -> {'*': 1722, '&': 4006, '(': 3021, ')': 2234, '$': 2714, \"'\": 3837, '#': 1969, '%': 3679, ',': 915, '-': 592, '4': 88, '7': 47, '+': 1141, '\"': 1290, '.': 427, '/': 342, '!': 795, '8': 36, '6': 48, '3': 121, '1': 219, '0': 248, '2': 156, '@': 7, '=': 10, '5': 70, '9': 37, ';': 20, ':': 24, '<': 23, '?': 13, 'O': 1, '>': 7, 'B': 5, 'K': 1, 'M': 2, 'C': 5, 'A': 4, 'T': 1, 'F': 4, 'G': 1, 'E': 2, 'D': 3, 'R': 1, 'J': 2, 'I': 1}\n",
      "র -> {'%': 2622, '*': 2367, '(': 3513, '!': 620, \"'\": 3682, ')': 2910, '&': 3236, '/': 502, ',': 1289, '.': 677, '$': 1831, '+': 1657, '-': 900, '2': 217, '#': 1408, '1': 253, '0': 364, '<': 17, '\"': 970, '4': 121, '7': 48, ';': 26, '5': 84, 'A': 8, '3': 135, '8': 44, '6': 87, '=': 13, '9': 28, 'K': 2, 'F': 2, 'J': 3, ':': 29, 'C': 6, '>': 9, 'm': 1, 'X': 1, 'D': 5, 'T': 1, '?': 5, 'P': 1, 'H': 1, 'S': 2, 'G': 1, 'B': 6, '@': 5, 'M': 1, 'W': 1, 'I': 1}\n",
      "▁এবং -> {'+': 2316, '(': 3500, '*': 2860, '-': 1331, ')': 3284, \"'\": 2993, '#': 1009, '$': 1237, '\"': 713, ',': 1805, '%': 1741, '&': 2381, '2': 303, '0': 535, '/': 704, '6': 97, '.': 985, '4': 177, '1': 346, '!': 460, '5': 111, '3': 228, '8': 47, '9': 46, '<': 23, ';': 26, '7': 64, 'J': 4, '>': 12, 'B': 5, 'F': 2, '=': 16, '@': 11, 'A': 6, ':': 39, '?': 18, 'D': 4, 'C': 4, 'N': 1, 'G': 3, 'I': 2, 'Y': 1, 'T': 1, 'O': 1, 'E': 1}\n",
      "▁ও -> {'+': 2808, ')': 3100, '-': 1862, ',': 2306, '(': 2921, \"'\": 2366, '&': 1638, '.': 1386, '*': 3115, '2': 376, '%': 1226, '5': 168, '#': 732, '\"': 474, '0': 758, '/': 961, '!': 377, '$': 887, ';': 50, '3': 307, '<': 32, '4': 245, '1': 571, '9': 50, '7': 90, '6': 128, 'J': 3, '?': 15, 'N': 3, ':': 42, '=': 31, 'A': 12, '8': 77, 'E': 5, '>': 15, 'G': 6, 'H': 4, 'C': 9, '@': 12, 'B': 11, 'F': 3, 'M': 2, 'D': 3, 'L': 1}\n",
      "; -> {'/': 1433, ',': 2634, '0': 1045, '*': 3087, ')': 2676, '+': 2962, '&': 1160, '-': 2318, '3': 452, \"'\": 1675, '7': 141, '%': 845, '4': 363, '.': 1787, '5': 238, '\"': 373, '1': 779, '$': 631, '6': 172, '(': 2247, '!': 301, '2': 568, '8': 107, '#': 521, ':': 67, ';': 40, '@': 15, '9': 74, '>': 33, 'H': 5, '<': 41, '?': 19, '=': 28, 'I': 4, 'C': 7, 'D': 6, 'F': 2, 'A': 11, 'O': 1, 'G': 3, 'B': 12, 'E': 4, 'J': 2, 'L': 2, 'M': 3, 'R': 1, 'c': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "mapped_tokens_file = \"fast_align/aligned_output.txt\"\n",
    "source_tokenizer = \"gpt2\"\n",
    "sentencepiece_model = \"bengali_sp.model\"\n",
    "\n",
    "tokenized_possible_translations = map_tokens(\n",
    "    mapped_tokens_file, source_tokenizer, sentencepiece_model\n",
    ")\n",
    "\n",
    "# Print a sample of the token mappings\n",
    "for new_token, mappings in list(tokenized_possible_translations.items())[:10]:\n",
    "    print(f\"{new_token} -> {dict(mappings)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d9357aae-92f0-48fe-8b57-486a25bcb82a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'old_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m english_words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGod\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mearth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Convert words to tokens using the GPT-2 tokenizer\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m english_tokens \u001b[38;5;241m=\u001b[39m [old_tokenizer\u001b[38;5;241m.\u001b[39mencode(word, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m english_words]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Print out the mappings for these tokens\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, token_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(english_words, english_tokens):\n",
      "Cell \u001b[0;32mIn[66], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m english_words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGod\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mearth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Convert words to tokens using the GPT-2 tokenizer\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m english_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mold_tokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mencode(word, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m english_words]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Print out the mappings for these tokens\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, token_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(english_words, english_tokens):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'old_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Key English words to check\n",
    "english_words = [\"God\", \"earth\", \"light\"]\n",
    "\n",
    "# Convert words to tokens using the GPT-2 tokenizer\n",
    "english_tokens = [old_tokenizer.encode(word, add_special_tokens=False)[0] for word in english_words]\n",
    "\n",
    "# Print out the mappings for these tokens\n",
    "for word, token_id in zip(english_words, english_tokens):\n",
    "    token = old_tokenizer.decode([token_id])\n",
    "    if token in tokenized_possible_translations:\n",
    "        print(f\"English word '{word}' is mapped to Bengali tokens: {tokenized_possible_translations[token]}\")\n",
    "    else:\n",
    "        print(f\"English word '{word}' has no mappings in the current alignments.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8ce029a0-0b6b-4112-828e-55c5617ac55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Tokens: [818, 262, 3726, 11, 1793, 2727, 262, 27636, 290, 262, 4534, 13]\n",
      "Bengali Tokens: [48071, 228, 48071, 99, 48071, 123, 48071, 97, 156, 100, 229, 220, 48071, 230, 48071, 114, 156, 100, 235, 48071, 105, 48071, 108, 220, 48071, 228, 48071, 243, 48071, 122, 48071, 114, 48071, 106, 48071, 96, 156, 100, 235, 48071, 94, 48071, 110, 220, 48071, 241, 220, 48071, 103, 156, 100, 225, 48071, 98, 48071, 123, 48071, 105, 156, 100, 222, 48071, 108, 220, 48071, 116, 156, 100, 225, 48071, 115, 156, 100, 235, 48071, 253, 48071, 123, 220, 48071, 243, 48071, 108, 48071, 110, 156, 100, 229, 48071, 101, 24231, 97]\n",
      "Decoded English: In the beginning, God created the heavens and the earth.\n",
      "Decoded Bengali: আদিতে ঈশ্বর আকাশমণ্ডল ও পৃথিবীর সৃষ্টি করলেন।\n"
     ]
    }
   ],
   "source": [
    "#using tiktoken library \n",
    "import tiktoken\n",
    "\n",
    "# Load the tokenizer for English (GPT-2 for instance)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Encode an example English sentence\n",
    "english_sentence = \"In the beginning, God created the heavens and the earth.\"\n",
    "english_tokens = enc.encode(english_sentence)\n",
    "print(f\"English Tokens: {english_tokens}\")\n",
    "\n",
    "# For Bengali, if you have a custom vocabulary or want to use a specific encoding\n",
    "# You may need to create a custom tokenizer, here's a conceptual example:\n",
    "\n",
    "# Assuming you have a Bengali vocab or want to use a pre-trained model's vocab\n",
    "# For this example, we create a simple Bengali tokenizer\n",
    "\n",
    "bengali_sentence = \"আদিতে ঈশ্বর আকাশমণ্ডল ও পৃথিবীর সৃষ্টি করলেন।\"\n",
    "bengali_tokens = enc.encode(bengali_sentence)\n",
    "print(f\"Bengali Tokens: {bengali_tokens}\")\n",
    "\n",
    "# Decode tokens back to text to verify\n",
    "decoded_english = enc.decode(english_tokens)\n",
    "decoded_bengali = enc.decode(bengali_tokens)\n",
    "\n",
    "print(f\"Decoded English: {decoded_english}\")\n",
    "print(f\"Decoded Bengali: {decoded_bengali}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58dbb0b4-1b8c-41c3-94fe-cfeeacf56150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create aligned corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ef28af72-1e53-4acc-a4d3-cf00e5b942eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Encoding.__init__() missing 1 required keyword-only argument: 'mergeable_ranks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m cleaned_parallel_corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_parallel_corpus.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Path to your cleaned parallel corpus file\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Define your custom Bengali encoding\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m tiktoken\u001b[38;5;241m.\u001b[39mTIKTOKEN_ENCODINGS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_bengali\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtiktoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncoding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustom_bengali\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpat_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myour_custom_pattern_here\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# You would need to provide the appropriate pattern here\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<|endoftext|>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50256\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m aligned_corpus_path \u001b[38;5;241m=\u001b[39m create_aligned_corpus(\n\u001b[1;32m     58\u001b[0m     source_language\u001b[38;5;241m=\u001b[39msource_language,\n\u001b[1;32m     59\u001b[0m     target_language\u001b[38;5;241m=\u001b[39mtarget_language,\n\u001b[1;32m     60\u001b[0m     cleaned_parallel_corpus\u001b[38;5;241m=\u001b[39mcleaned_parallel_corpus\n\u001b[1;32m     61\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAligned corpus created at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maligned_corpus_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Encoding.__init__() missing 1 required keyword-only argument: 'mergeable_ranks'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "\n",
    "ALIGNMENT_UNIT = \"WORDS\"  # Ensuring alignment at the word level\n",
    "home_path = Path(\"tiktoken\")  # Directory for storing the outputs\n",
    "\n",
    "def create_aligned_corpus(\n",
    "    source_language: str,\n",
    "    target_language: str,\n",
    "    cleaned_parallel_corpus: str,\n",
    "    source_encoding: str = \"gpt2\",  # Encoding name for English\n",
    "    target_encoding: str = \"custom_bengali\"  # Encoding name for Bengali\n",
    "):\n",
    "    # Load the tokenizers (encodings) for both languages\n",
    "    enc_source = tiktoken.get_encoding(source_encoding)\n",
    "    enc_target = tiktoken.get_encoding(target_encoding)\n",
    "\n",
    "    # Define the output path for the Moses-style file\n",
    "    out_path = f'{home_path}/aligned_corpus.moses'\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(out_path):\n",
    "        print(f'Data already preprocessed for fast_align')\n",
    "    else:\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(f'{home_path}/alignments', exist_ok=True)\n",
    "        \n",
    "        # Open the parallel corpus and the output file\n",
    "        with open(cleaned_parallel_corpus, 'r', encoding='utf-8') as infile, open(out_path, 'w', encoding='utf-8') as outfile:\n",
    "            for line in tqdm(infile):\n",
    "                if '|||' in line:\n",
    "                    line_source, line_target = line.strip().split(' ||| ')\n",
    "                    \n",
    "                    # Tokenize the sentences\n",
    "                    line1 = ' '.join(map(str, enc_source.encode(line_source.strip())))\n",
    "                    line2 = ' '.join(map(str, enc_target.encode(line_target.strip())))\n",
    "\n",
    "                    # Write the tokenized and aligned lines to the output file\n",
    "                    outfile.write(line1.strip() + ' ||| ' + line2.strip() + '\\n')\n",
    "\n",
    "    return out_path\n",
    "\n",
    "# Example usage\n",
    "source_language = \"en\"\n",
    "target_language = \"bn\"\n",
    "cleaned_parallel_corpus = \"cleaned_parallel_corpus.txt\"  # Path to your cleaned parallel corpus file\n",
    "\n",
    "# Define your custom Bengali encoding\n",
    "tiktoken.TIKTOKEN_ENCODINGS[\"custom_bengali\"] = tiktoken.Encoding(\n",
    "    name=\"custom_bengali\",\n",
    "    pat_str=r\"your_custom_pattern_here\",  # You would need to provide the appropriate pattern here\n",
    "    special_tokens={\"<|endoftext|>\": 50256},\n",
    ")\n",
    "\n",
    "aligned_corpus_path = create_aligned_corpus(\n",
    "    source_language=source_language,\n",
    "    target_language=target_language,\n",
    "    cleaned_parallel_corpus=cleaned_parallel_corpus\n",
    ")\n",
    "\n",
    "print(f\"Aligned corpus created at: {aligned_corpus_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f90945ab-744a-4af5-9932-8a893e927065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mergeable ranks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2edb53-5457-43be-b4e5-7877343631f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Train a SentencePiece model (assuming corpus.txt is your Bengali text corpus)\n",
    "spm.SentencePieceTrainer.train(input='corpus.txt', model_prefix='bengali_bpe', vocab_size=32000)\n",
    "\n",
    "# Load the trained model\n",
    "sp = spm.SentencePieceProcessor(model_file='bengali_bpe.model')\n",
    "\n",
    "# Example: Get the mergeable ranks from the SentencePiece model\n",
    "mergeable_ranks = {pair: i for i, pair in enumerate(sp.get_vocabulary())}\n",
    "\n",
    "# Print out the first few mergeable ranks\n",
    "print(list(mergeable_ranks.items())[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
