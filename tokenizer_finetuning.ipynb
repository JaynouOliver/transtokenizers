{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91d38bcd-23c8-4101-9c86-f418943f1c46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug  7 11:42:18 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  |   00000000:3F:00.0 Off |                  Off |\n",
      "| 30%   27C    P8             22W /  300W |       3MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392aae20-4e83-4ae0-9f96-a2b15daf8679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a bangla tokenizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8f3bf6-123b-44aa-8f22-de75c09b536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the llama model on the bangla dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47e3a570-f86f-49d4-b29a-57bb7c600ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9eaa35b28e249469b6b1c698df31f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51ce96bd-bf0e-4ebc-a663-5ec1a6c868bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-oy39idnk/unsloth_e38a356a46b7450d8223692acfe7bd7d\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-oy39idnk/unsloth_e38a356a46b7450d8223692acfe7bd7d\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 637ed8c6bd252f981e89e30e1085efc03a06a880\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.2)\n",
      "Requirement already satisfied: tyro in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.7.3)\n",
      "Requirement already satisfied: transformers>=4.43.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.43.4)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.1)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.2)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.8)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.44.0)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.24.5)\n",
      "Requirement already satisfied: hf-transfer in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.6)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.13.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.31.0)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from huggingface-hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.10.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers>=4.43.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers>=4.43.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers>=4.43.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.7.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.17.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: xformers<0.0.27 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.0.23.post1+cu118)\n",
      "Requirement already satisfied: trl<0.9.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.8.6)\n",
      "Requirement already satisfied: peft in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.12.1.dev0)\n",
      "Requirement already satisfied: accelerate in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.32.0)\n",
      "Requirement already satisfied: bitsandbytes in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.43.3)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (2.19.1)\n",
      "Requirement already satisfied: transformers in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (4.43.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#download unsloth\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes datasets transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bbe24c0-8785-40c9-9bd6-1edf1428cefc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (2.19.1)\n",
      "Requirement already satisfied: tokenizers in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.19.1)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (4.43.4)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.24.5)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers[sentencepiece]) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.4.2)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers[sentencepiece]) (3.20.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install datasets tokenizers transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68c14543-11a7-4a79-9a23-45c975177aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers.normalizers import NFD, StripAccents, Replace, Sequence\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers, processors, decoders\n",
    "import re\n",
    "from tokenizers import Regex\n",
    "from tokenizers.normalizers import NFD, StripAccents, Replace, Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9501f3d1-bc97-4bda-8856-e12bc155abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from huggingface_hub import HfApi, list_models\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "from peft import get_peft_model, LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "494f2b7f-23bf-4e2a-9bef-f3272ba63195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn>=2.6.3\n",
      "  Downloading flash_attn-2.6.3.tar.gz (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.6.3-cp310-cp310-linux_x86_64.whl size=188977699 sha256=d16f6008f2b7727718653bfd087a20f3e18e2a4c8e91b9f38da2439f3b0bf529\n",
      "  Stored in directory: /root/.cache/pip/wheels/7e/e3/c3/89c7a2f3c4adc07cd1c675f8bb7b9ad4d18f64a72bccdfe826\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: flash-attn\n",
      "  Attempting uninstall: flash-attn\n",
      "    Found existing installation: flash-attn 2.5.5\n",
      "    Uninstalling flash-attn-2.5.5:\n",
      "      Successfully uninstalled flash-attn-2.5.5\n",
      "Successfully installed flash-attn-2.6.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --no-deps --upgrade \"flash-attn>=2.6.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51051ef8-5cd6-4f84-8454-80a7669684ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for oscar contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/oscar\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d74cd6a6ae44a0bdf0a5b7d9c73ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/14.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ce094fc8004a54959f59720da22ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/303k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef57df1f35234c3d838ba7a2d0ec5554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/328 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe2a9c8f7c34808a8515d2c6e0a4b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/390M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00513e0a90f94e2cb88649510a2a8a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/390M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5f27a2ef0a42c9bb7d04978fe9731f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/391M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a25227590b44bd8147cfa6b265c9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/86.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a58f1f5a4db496192b06fef9d0f2b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1114481 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load bangla dataset\n",
    "dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_bn\", split=\"train\")\n",
    "\n",
    "#link to dataset  = https://huggingface.co/datasets/oscar-corpus/oscar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a08ae0c9-67f2-44d1-a331-5b2005cb3d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9196f38a-50d1-4fbc-8223-36ce414c2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE tokenizer \n",
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4981263-25c1-40c5-addf-e80d6c73a2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.NFD(),\n",
    "    normalizers.StripAccents(),\n",
    "    normalizers.Replace(Regex(r\"[\\s]+\"), \" \"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa677e99-daa2-44a7-815c-b6d9cd6c99d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set normalizer\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "037660ed-eaf3-45a2-8761-91d01d870978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "trainer = trainers.BpeTrainer(vocab_size=30000, special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0040bf1-d538-4f07-91de-1ac60a2f4008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
    "\n",
    "# Save\n",
    "tokenizer.save(\"bangla_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02be0a59-4b12-428f-b1e1-75f80ac6a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "bangla_tokenizer = Tokenizer.from_file(\"bangla_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c6d55e5-cc07-4a98-8a8d-9a74c24f9bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the tokenizer\n",
    "bangla_tokenizer = PreTrainedTokenizerFast(tokenizer_object=bangla_tokenizer, unk_token=\"<unk>\", pad_token=\"<pad>\", cls_token=\"<s>\", sep_token=\"</s>\", mask_token=\"<mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "feae6ee9-5d7a-475c-9f1f-c800a172ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(examples):\n",
    "    texts = examples[\"text\"]\n",
    "    # Tokenize the texts\n",
    "    tokenized_texts = bangla_tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    return {\"input_ids\": tokenized_texts[\"input_ids\"], \"attention_mask\": tokenized_texts[\"attention_mask\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bb28b7d-9b35-4fe6-952d-6f9d6988399d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for oscar contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/oscar\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bd63e09e7b499587d9b59c853b7e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and format the dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_bn\", split=\"train[:1%]\")\n",
    "dataset = dataset.map(format_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c743ec2d-c0d9-49e9-b455-d09b2bf35c71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.dev0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.529 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.1.2+cu118. CUDA = 8.6. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.23.post1+cu118. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea94e2d37474e759dac123e493dae35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3bf7ac31dc043c2aa7ff2a397bf853e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eea304809ff405481b4d1711fae70eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39683198c1e5464cb1e7ec3a936a0b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2e66fb2f0d468f8c40b591dbcc9d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "511436bf-4d47-484a-89ed-f382b5cb1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach LoRA Adapters\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be47690d-67df-4f75-9f11-08bcffc64bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=bangla_tokenizer,\n",
    "    mlm=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5c818c8-83da-4730-a373-7fb032058d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bangla_llama\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10_000,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f15dc8f6-fe00-4c4c-9686-884ce57a5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a7f7b1-416e-48a3-954b-c962f0095e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with progress bar\n",
    "wandb.init(project=\"bangla-llama-finetuning\")\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{training_args.num_train_epochs}\")\n",
    "    trainer.train()\n",
    "\n",
    "# Save the model with progress bar\n",
    "print(\"Saving the model...\")\n",
    "trainer.save_model(\"./bangla_llama\")\n",
    "bangla_tokenizer.save(\"bangla_llama\")\n",
    "print(\"Model saved.\")\n",
    "\n",
    "# Finish Weights & Biases run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23d3b385-f460-4e40-b3f5-8214d5b7254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final workign code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f4772-13d4-4c3c-b2e3-d577abf3f982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "WARNING: BNB_CUDA_VERSION=118 environment variable detected; loading libbitsandbytes_cuda118.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for oscar contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/oscar\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.4.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.529 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.1.2+cu118. CUDA = 8.6. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.23.post1+cu118. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "[2024-08-07 13:33:44,329] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 11,145 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 4 | Total steps = 8,361\n",
      " \"-____-\"     Number of trainable parameters = 6,815,744\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msuvrakamal\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/transtokenizers/wandb/run-20240807_133346-2cfxf3ek</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/suvrakamal/huggingface/runs/2cfxf3ek/workspace' target=\"_blank\">./bangla_llama</a></strong> to <a href='https://wandb.ai/suvrakamal/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/suvrakamal/huggingface' target=\"_blank\">https://wandb.ai/suvrakamal/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/suvrakamal/huggingface/runs/2cfxf3ek/workspace' target=\"_blank\">https://wandb.ai/suvrakamal/huggingface/runs/2cfxf3ek/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8361' max='8361' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8361/8361 1:49:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 11,145 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 4 | Total steps = 8,361\n",
      " \"-____-\"     Number of trainable parameters = 6,815,744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8361' max='8361' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8361/8361 1:49:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 11,145 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 4 | Total steps = 8,361\n",
      " \"-____-\"     Number of trainable parameters = 6,815,744\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2517' max='8361' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2517/8361 32:47 < 1:16:11, 1.28 it/s, Epoch 0.90/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import wandb\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "# wandb.init(project=\"bangla-llama-finetuning\")\n",
    "\n",
    "# Load the custom Bangla tokenizer\n",
    "bangla_tokenizer = Tokenizer.from_file(\"bangla_tokenizer.json\")\n",
    "\n",
    "# Wrap the tokenizer\n",
    "bangla_tokenizer = PreTrainedTokenizerFast(tokenizer_object=bangla_tokenizer, unk_token=\"<unk>\", pad_token=\"<pad>\", cls_token=\"<s>\", sep_token=\"</s>\", mask_token=\"<mask>\")\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_bn\", split=\"train[:1%]\")\n",
    "\n",
    "# Function to format the dataset for training\n",
    "def format_dataset(examples):\n",
    "    texts = examples[\"text\"]\n",
    "    # Tokenize the texts\n",
    "    tokenized_texts = bangla_tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    return {\"input_ids\": tokenized_texts[\"input_ids\"], \"attention_mask\": tokenized_texts[\"attention_mask\"]}\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "dataset = dataset.map(format_dataset, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "dataset = dataset.remove_columns([\"id\", \"text\"])\n",
    "\n",
    "# Initialize the model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# Attach LoRA Adapters\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=bangla_tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8 \n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bangla_llama\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10_000,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    # report_to=\"wandb\",\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset\n",
    ")\n",
    "\n",
    "# Train the model with progress bar\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{training_args.num_train_epochs}\")\n",
    "    trainer.train()\n",
    "\n",
    "# Save the model with progress bar\n",
    "print(\"Saving the model...\")\n",
    "trainer.save_model(\"./bangla_llama\")\n",
    "bangla_tokenizer.save(\"bangla_llama\")\n",
    "print(\"Model saved.\")\n",
    "\n",
    "# Finish Weights & Biases run\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f40b04b-2ba0-41e7-be61-5995c354a052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for oscar contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/oscar\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.4.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.256 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.1.2+cu118. CUDA = 8.0. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.23.post1+cu118. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "[2024-08-08 15:34:33,813] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 11,145 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 16 | Total steps = 2,091\n",
      " \"-____-\"     Number of trainable parameters = 6,815,744\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msuvrakamal\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240808_153435-tt32zgs9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/suvrakamal/huggingface/runs/tt32zgs9/workspace' target=\"_blank\">./new_model</a></strong> to <a href='https://wandb.ai/suvrakamal/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/suvrakamal/huggingface' target=\"_blank\">https://wandb.ai/suvrakamal/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/suvrakamal/huggingface/runs/tt32zgs9/workspace' target=\"_blank\">https://wandb.ai/suvrakamal/huggingface/runs/tt32zgs9/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2091' max='2091' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2091/2091 45:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 11,145 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 16 | Total steps = 2,091\n",
      " \"-____-\"     Number of trainable parameters = 6,815,744\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2091' max='2091' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2091/2091 43:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 11,145 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 16 | Total steps = 2,091\n",
      " \"-____-\"     Number of trainable parameters = 6,815,744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2091' max='2091' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2091/2091 43:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PreTrainedTokenizerFast' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving the model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./new_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m \u001b[43mbangla_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Finish Weights & Biases run\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# wandb.finish()\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PreTrainedTokenizerFast' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "# import wandb\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "# wandb.init(project=\"bangla-llama-finetuning\")\n",
    "\n",
    "# Load the custom Bangla tokenizer\n",
    "bangla_tokenizer = Tokenizer.from_file(\"bangla_tokenizer.json\")\n",
    "\n",
    "# Wrap the tokenizer\n",
    "bangla_tokenizer = PreTrainedTokenizerFast(tokenizer_object=bangla_tokenizer, unk_token=\"<unk>\", pad_token=\"<pad>\", cls_token=\"<s>\", sep_token=\"</s>\", mask_token=\"<mask>\")\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_bn\", split=\"train[:1%]\")\n",
    "\n",
    "# Function to format the dataset for training\n",
    "def format_dataset(examples):\n",
    "    texts = examples[\"text\"]\n",
    "    # Tokenize the texts\n",
    "    tokenized_texts = bangla_tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    return {\"input_ids\": tokenized_texts[\"input_ids\"], \"attention_mask\": tokenized_texts[\"attention_mask\"]}\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "dataset = dataset.map(format_dataset, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "dataset = dataset.remove_columns([\"id\", \"text\"])\n",
    "\n",
    "# Initialize the model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# Attach LoRA Adapters\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=bangla_tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8 \n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./new_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10_000,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    # report_to=\"wandb\",\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset\n",
    ")\n",
    "\n",
    "# Train the model with progress bar\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{training_args.num_train_epochs}\")\n",
    "    trainer.train()\n",
    "\n",
    "# Save the model with progress bar\n",
    "print(\"Saving the model...\")\n",
    "trainer.save_model(\"./new_model\")\n",
    "bangla_tokenizer.save(\"new_model\")\n",
    "print(\"Model saved.\")\n",
    "\n",
    "# Finish Weights & Biases run\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7924ddb3-0ea6-4489-8a34-f15c16741188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model...\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving the model...\")\n",
    "trainer.save_model(\"./new_model\")\n",
    "bangla_tokenizer.save_pretrained(\"new_model\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b97e7-b82b-450f-a6c1-31cd92f0f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers peft --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de476274-2b59-4ee9-9327-94c5482307a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "basemodel_path = \"unsloth/Meta-Llama-3.1-8B\"\n",
    "loramodel_path = \"bangla_llama\"\n",
    "hf_token = \"HF_TOKEN\"\n",
    "output_model_name = \"bangla_toknizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba4ee2d-7f25-41d3-b204-879ef8561e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "import huggingface_hub as hf_hub\n",
    "\n",
    "hf_hub.login(hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d42c75-3bda-4375-bc8a-b51ed0634ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(basemodel_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(basemodel_path, trust_remote_code=True, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07091d1c-a949-4f35-90dd-3c86659a803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and merge LoRA model\n",
    "if loramodel_path:\n",
    "    lora_model = PeftModel.from_pretrained(model, loramodel_path)\n",
    "    model = lora_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a465d-56c0-4dd7-a2c6-d314ec190680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer and model locally\n",
    "tokenizer.save_pretrained(output_model_name)\n",
    "model.save_pretrained(output_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6176e9d-1b5e-44bf-bb3f-f12a4662b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to Hugging Face Hub\n",
    "repo = hf_hub.create_repo(output_model_name, private=False)  # Set private=False if you want it to be public\n",
    "hf_hub.upload_folder(\n",
    "    folder_path=output_model_name,\n",
    "    path_in_repo='',  # Root of the repo\n",
    "    repo_id=f\"{hf_hub.whoami()['name']}/{output_model_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2eca6-bdc1-4246-9ac8-3ee03d858fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
