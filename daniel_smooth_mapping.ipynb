{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "def read_alignment_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        alignments = file.readlines()\n",
    "    return alignments\n",
    "\n",
    "forward_alignments = read_alignment_file('forward.align')\n",
    "reverse_alignments = read_alignment_file('reverse.align')\n",
    "bidirectional_alignments = read_alignment_file('bidirectional.align')\n",
    "\n",
    "def create_token_mapping(alignments):\n",
    "    token_map = defaultdict(lambda: defaultdict(int))\n",
    "    for line in alignments:\n",
    "        parts = line.strip().split()\n",
    "        for part in parts:\n",
    "            en_idx, bn_idx = map(int, part.split('-'))\n",
    "            token_map[en_idx][bn_idx] += 1\n",
    "    return token_map\n",
    "\n",
    "token_map = create_token_mapping(bidirectional_alignments)\n",
    "\n",
    "# Filter low-frequency alignments\n",
    "MIN_FREQUENCY = 5\n",
    "filtered_token_map = {\n",
    "    en_idx: {bn_idx: freq for bn_idx, freq in alignments.items() if freq >= MIN_FREQUENCY}\n",
    "    for en_idx, alignments in token_map.items()\n",
    "}\n",
    "\n",
    "print(filtered_token_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_mappings(token_map, vocab_size, smoothing_factor=1e-5):\n",
    "    smoothed_map = {}\n",
    "    for en_idx, alignments in token_map.items():\n",
    "        if not alignments:\n",
    "            # Uniform distribution if no alignments\n",
    "            smoothed_map[en_idx] = [1.0 / vocab_size] * vocab_size\n",
    "        else:\n",
    "            total = sum(alignments.values())\n",
    "            # Initialize with a small probability mass\n",
    "            smoothed_map[en_idx] = [smoothing_factor] * vocab_size\n",
    "            # Add the observed probabilities\n",
    "            for bn_idx, freq in alignments.items():\n",
    "                smoothed_map[en_idx][int(bn_idx)] += freq / total\n",
    "            # Renormalize\n",
    "            total_prob = sum(smoothed_map[en_idx])\n",
    "            smoothed_map[en_idx] = [prob / total_prob for prob in smoothed_map[en_idx]]\n",
    "    return smoothed_map\n",
    "\n",
    "bn_vocab_size = 32000  # Set this to the size of your Bengali vocabulary\n",
    "smoothed_mappings = smooth_mappings(filtered_token_map, bn_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a small example\n",
    "test_token_map = {\n",
    "    0: {0: 10, 1: 5},\n",
    "    1: {1: 15, 2: 5},\n",
    "    2: {}  # No alignments for this token\n",
    "}\n",
    "\n",
    "test_vocab_size = 5\n",
    "test_smoothed_mappings = smooth_mappings(test_token_map, test_vocab_size)\n",
    "\n",
    "for en_idx, probs in test_smoothed_mappings.items():\n",
    "    print(f\"Token {en_idx}: {probs}\")\n",
    "    print(f\"Sum of probabilities: {sum(probs)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_vocab_size = 32000  # Set this to the size of your Bengali vocabulary\n",
    "smoothed_mappings = smooth_mappings(filtered_token_map, bn_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of tokens mapped: {len(smoothed_mappings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, probs in list(smoothed_mappings.items())[:91]:  # Check first 10 tokens\n",
    "    print(f\"Token {token} sum: {sum(probs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in [0, 1, 10, 100, 1000]:  # Check some specific token ids\n",
    "    if token in smoothed_mappings:\n",
    "        top_5 = sorted(enumerate(smoothed_mappings[token]), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(f\"Token {token} top 5 mappings: {top_5}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
